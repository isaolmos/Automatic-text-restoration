{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Wordsegmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOQ8U6XpsaHk",
        "outputId": "47cb3988-57c8-4d57-de3a-9675af21f3b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVsUySSrpHw-",
        "outputId": "a179154e-a237-4af1-f525-71fe3f3562e2"
      },
      "source": [
        "cd drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efZ2j9t9ObKB",
        "outputId": "d7a529e6-752e-4854-95a3-1fc3a0b77062"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SHxy9qkeaSs",
        "outputId": "9aaa828c-a8ae-43f1-b658-25eaa095859b"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 62  # batch size\n",
        "latent_dim = 128  # latent dimensionality of the encoding space\n",
        "num_samples = 10000  # number of samples to train on\n",
        "\n",
        "# data path\n",
        "data_path = \"train_y.csv\"\n",
        "\n",
        "# vectorize the data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "# set of unique characters\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# reading in data removing spaces for train x\n",
        "# converting train x into train y by converting each character in each word into 0 and 1\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    target_lines = f.read().split(\"\\n\")\n",
        "for input_text, target_text in zip(lines, target_lines):\n",
        "    input_text = ''.join(input_text.split()[:])\n",
        "    target_text = ''.join(['1' + '0' * (len(word) -1) for word in target_text.split()[:6]])\n",
        "    \n",
        "    # \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    input_characters.add(' ')\n",
        "    target_characters.add(' ')\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "# converts the alphebet set to list of charactesr and sorts them\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# makes a note of the number of input and output tokens\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "# max input and output length in the train set\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "# the decoded characeter are 1 signalling the start of a word, and 0 signalling inside a word\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# this allows you to look up the character to find its indice\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# empty arrays for encoding\n",
        "# space for every character 834(max sent length) x27\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "# one hot encoding\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "\n",
        "    # decoder output is fed back into the decoder input       \n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# building the model\n",
        "# define an input sequence to process it\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.Bidirectional(keras.layers.LSTM(latent_dim, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "state_h = keras.layers.Concatenate()([forward_h, backward_h])\n",
        "state_c = keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# discard encoder_outputs and only keep the states\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# set up the decoder using encoder_states as initial state\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# we set up our decoder to return full output sequences, and to return internal states as well \n",
        "# we don't use the return states in the training model, but we will use them in inference\n",
        "decoder_lstm = keras.layers.LSTM(2*latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# define the model that will turn\n",
        "# encoder_input_data and decoder_input_data into decoder_target_data\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# train the model\n",
        "opt = keras.optimizers.Adam(0.003)\n",
        "model.compile(\n",
        "    optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall(), tf.keras.metrics.Precision()] \n",
        ")\n",
        "epochs = 40\n",
        "model_data = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# saving the model if need to access the best result\n",
        "model.save(\"word_seg8\")\n",
        "\n",
        "# calculating the f score for the final epoch\n",
        "Precision = model_data.history['val_precision'][-1]\n",
        "Recall = model_data.history['val_recall'][-1]\n",
        "\n",
        "f1score = 2*Precision*Recall/(Precision+Recall)\n",
        "print(\"F1 score \", f1score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 49374\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 5\n",
            "Max sequence length for inputs: 834\n",
            "Max sequence length for outputs: 63\n",
            "Epoch 1/40\n",
            "638/638 [==============================] - 89s 127ms/step - loss: 0.2426 - accuracy: 0.9035 - recall: 0.8969 - precision: 0.9073 - val_loss: 0.2146 - val_accuracy: 0.9137 - val_recall: 0.9136 - val_precision: 0.9139\n",
            "Epoch 2/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.2175 - accuracy: 0.9130 - recall: 0.9125 - precision: 0.9134 - val_loss: 0.1866 - val_accuracy: 0.9221 - val_recall: 0.9220 - val_precision: 0.9223\n",
            "Epoch 3/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.1796 - accuracy: 0.9209 - recall: 0.9204 - precision: 0.9213 - val_loss: 0.1500 - val_accuracy: 0.9318 - val_recall: 0.9304 - val_precision: 0.9330\n",
            "Epoch 4/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.1539 - accuracy: 0.9310 - recall: 0.9305 - precision: 0.9315 - val_loss: 0.1456 - val_accuracy: 0.9327 - val_recall: 0.9321 - val_precision: 0.9331\n",
            "Epoch 5/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.1327 - accuracy: 0.9417 - recall: 0.9413 - precision: 0.9421 - val_loss: 0.1165 - val_accuracy: 0.9506 - val_recall: 0.9499 - val_precision: 0.9511\n",
            "Epoch 6/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.1143 - accuracy: 0.9513 - recall: 0.9510 - precision: 0.9514 - val_loss: 0.1088 - val_accuracy: 0.9536 - val_recall: 0.9524 - val_precision: 0.9548\n",
            "Epoch 7/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.1004 - accuracy: 0.9587 - recall: 0.9584 - precision: 0.9589 - val_loss: 0.0880 - val_accuracy: 0.9662 - val_recall: 0.9660 - val_precision: 0.9663\n",
            "Epoch 8/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0867 - accuracy: 0.9652 - recall: 0.9651 - precision: 0.9653 - val_loss: 0.0834 - val_accuracy: 0.9676 - val_recall: 0.9675 - val_precision: 0.9676\n",
            "Epoch 9/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0779 - accuracy: 0.9693 - recall: 0.9692 - precision: 0.9694 - val_loss: 0.0682 - val_accuracy: 0.9742 - val_recall: 0.9741 - val_precision: 0.9742\n",
            "Epoch 10/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0702 - accuracy: 0.9730 - recall: 0.9729 - precision: 0.9731 - val_loss: 0.0640 - val_accuracy: 0.9760 - val_recall: 0.9760 - val_precision: 0.9760\n",
            "Epoch 11/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0637 - accuracy: 0.9758 - recall: 0.9757 - precision: 0.9758 - val_loss: 0.0618 - val_accuracy: 0.9772 - val_recall: 0.9766 - val_precision: 0.9778\n",
            "Epoch 12/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0576 - accuracy: 0.9783 - recall: 0.9783 - precision: 0.9784 - val_loss: 0.0547 - val_accuracy: 0.9804 - val_recall: 0.9804 - val_precision: 0.9804\n",
            "Epoch 13/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0559 - accuracy: 0.9792 - recall: 0.9791 - precision: 0.9793 - val_loss: 0.0508 - val_accuracy: 0.9815 - val_recall: 0.9815 - val_precision: 0.9815\n",
            "Epoch 14/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0519 - accuracy: 0.9808 - recall: 0.9806 - precision: 0.9809 - val_loss: 0.0586 - val_accuracy: 0.9783 - val_recall: 0.9782 - val_precision: 0.9783\n",
            "Epoch 15/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0477 - accuracy: 0.9824 - recall: 0.9824 - precision: 0.9825 - val_loss: 0.0500 - val_accuracy: 0.9825 - val_recall: 0.9824 - val_precision: 0.9825\n",
            "Epoch 16/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0953 - accuracy: 0.9624 - recall: 0.9621 - precision: 0.9626 - val_loss: 0.1730 - val_accuracy: 0.9205 - val_recall: 0.9203 - val_precision: 0.9206\n",
            "Epoch 17/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.1545 - accuracy: 0.9309 - recall: 0.9307 - precision: 0.9311 - val_loss: 0.1100 - val_accuracy: 0.9512 - val_recall: 0.9509 - val_precision: 0.9514\n",
            "Epoch 18/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0984 - accuracy: 0.9582 - recall: 0.9581 - precision: 0.9583 - val_loss: 0.0764 - val_accuracy: 0.9696 - val_recall: 0.9695 - val_precision: 0.9697\n",
            "Epoch 19/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0724 - accuracy: 0.9707 - recall: 0.9707 - precision: 0.9708 - val_loss: 0.0594 - val_accuracy: 0.9775 - val_recall: 0.9774 - val_precision: 0.9775\n",
            "Epoch 20/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0592 - accuracy: 0.9770 - recall: 0.9769 - precision: 0.9770 - val_loss: 0.0580 - val_accuracy: 0.9783 - val_recall: 0.9782 - val_precision: 0.9783\n",
            "Epoch 21/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0520 - accuracy: 0.9802 - recall: 0.9801 - precision: 0.9802 - val_loss: 0.0482 - val_accuracy: 0.9825 - val_recall: 0.9824 - val_precision: 0.9825\n",
            "Epoch 22/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0468 - accuracy: 0.9824 - recall: 0.9824 - precision: 0.9825 - val_loss: 0.0471 - val_accuracy: 0.9830 - val_recall: 0.9830 - val_precision: 0.9831\n",
            "Epoch 23/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0435 - accuracy: 0.9839 - recall: 0.9839 - precision: 0.9839 - val_loss: 0.0472 - val_accuracy: 0.9831 - val_recall: 0.9831 - val_precision: 0.9831\n",
            "Epoch 24/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0416 - accuracy: 0.9848 - recall: 0.9847 - precision: 0.9848 - val_loss: 0.0477 - val_accuracy: 0.9836 - val_recall: 0.9836 - val_precision: 0.9837\n",
            "Epoch 25/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0393 - accuracy: 0.9857 - recall: 0.9857 - precision: 0.9857 - val_loss: 0.0423 - val_accuracy: 0.9852 - val_recall: 0.9852 - val_precision: 0.9852\n",
            "Epoch 26/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0372 - accuracy: 0.9865 - recall: 0.9865 - precision: 0.9865 - val_loss: 0.0404 - val_accuracy: 0.9861 - val_recall: 0.9860 - val_precision: 0.9861\n",
            "Epoch 27/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0356 - accuracy: 0.9871 - recall: 0.9871 - precision: 0.9871 - val_loss: 0.0409 - val_accuracy: 0.9858 - val_recall: 0.9858 - val_precision: 0.9859\n",
            "Epoch 28/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0335 - accuracy: 0.9880 - recall: 0.9880 - precision: 0.9880 - val_loss: 0.0406 - val_accuracy: 0.9860 - val_recall: 0.9860 - val_precision: 0.9860\n",
            "Epoch 29/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0658 - accuracy: 0.9788 - recall: 0.9787 - precision: 0.9789 - val_loss: 0.0487 - val_accuracy: 0.9828 - val_recall: 0.9828 - val_precision: 0.9829\n",
            "Epoch 30/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0364 - accuracy: 0.9869 - recall: 0.9869 - precision: 0.9869 - val_loss: 0.0384 - val_accuracy: 0.9867 - val_recall: 0.9867 - val_precision: 0.9868\n",
            "Epoch 31/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0305 - accuracy: 0.9892 - recall: 0.9892 - precision: 0.9892 - val_loss: 0.0376 - val_accuracy: 0.9874 - val_recall: 0.9873 - val_precision: 0.9874\n",
            "Epoch 32/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0288 - accuracy: 0.9897 - recall: 0.9897 - precision: 0.9897 - val_loss: 0.0380 - val_accuracy: 0.9875 - val_recall: 0.9875 - val_precision: 0.9875\n",
            "Epoch 33/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0280 - accuracy: 0.9901 - recall: 0.9901 - precision: 0.9901 - val_loss: 0.0379 - val_accuracy: 0.9875 - val_recall: 0.9875 - val_precision: 0.9875\n",
            "Epoch 34/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0275 - accuracy: 0.9903 - recall: 0.9903 - precision: 0.9903 - val_loss: 0.0388 - val_accuracy: 0.9871 - val_recall: 0.9871 - val_precision: 0.9871\n",
            "Epoch 35/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0271 - accuracy: 0.9905 - recall: 0.9905 - precision: 0.9905 - val_loss: 0.0391 - val_accuracy: 0.9871 - val_recall: 0.9871 - val_precision: 0.9871\n",
            "Epoch 36/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0261 - accuracy: 0.9908 - recall: 0.9908 - precision: 0.9909 - val_loss: 0.0367 - val_accuracy: 0.9877 - val_recall: 0.9877 - val_precision: 0.9877\n",
            "Epoch 37/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0260 - accuracy: 0.9908 - recall: 0.9908 - precision: 0.9909 - val_loss: 0.0366 - val_accuracy: 0.9876 - val_recall: 0.9876 - val_precision: 0.9876\n",
            "Epoch 38/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0258 - accuracy: 0.9909 - recall: 0.9909 - precision: 0.9909 - val_loss: 0.0385 - val_accuracy: 0.9875 - val_recall: 0.9875 - val_precision: 0.9875\n",
            "Epoch 39/40\n",
            "638/638 [==============================] - 78s 123ms/step - loss: 0.0251 - accuracy: 0.9911 - recall: 0.9911 - precision: 0.9912 - val_loss: 0.0371 - val_accuracy: 0.9877 - val_recall: 0.9876 - val_precision: 0.9877\n",
            "Epoch 40/40\n",
            "638/638 [==============================] - 79s 123ms/step - loss: 0.0244 - accuracy: 0.9915 - recall: 0.9915 - precision: 0.9915 - val_loss: 0.0396 - val_accuracy: 0.9872 - val_recall: 0.9871 - val_precision: 0.9872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: word_seg8/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: word_seg8/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1 score  0.9871510562469401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqtAF8GiTNXz",
        "outputId": "1b864bd9-61f2-4a8b-aaf8-36cf7ed3c5a6"
      },
      "source": [
        "# define sampling models\n",
        "# restore the model and construct the encoder and decoder\n",
        "model = keras.models.load_model(\"word_seg8\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "print(model.layers)\n",
        "print(model.layers[1].output)\n",
        "encoder_outputs, _,_,_,_= model.layers[1].output  # lstm_1\n",
        "state_h_enc = model.layers[3].output\n",
        "state_c_enc = model.layers[4].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim * 2,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim * 2,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[5]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[6]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# reverse-lookup token index to decode sequences back to something readable\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # populate the first character of target sequence with the start character\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # sampling loop for a batch of sequences\n",
        "    # to simplify, here we assume a batch of size 1\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # exit condition: either hit max length\n",
        "        # or find stop character\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f8398f86510>, <tensorflow.python.keras.layers.wrappers.Bidirectional object at 0x7f82ca4b6790>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f827c2bd8d0>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f82e620f310>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f827d484c10>, <tensorflow.python.keras.layers.recurrent_v2.LSTM object at 0x7f827d6ca110>, <tensorflow.python.keras.layers.core.Dense object at 0x7f827d7dbe10>]\n",
            "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'bidirectional')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'bidirectional')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'bidirectional')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'bidirectional')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'bidirectional')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGLQEtDM1CF_",
        "outputId": "25a33023-7265-4375-c025-32a82a317610"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # take one sequence (part of the training set) for decoding\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    input_text = input_texts[seq_index]\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_text)\n",
        "    print(\"Decoded sentence:\", end='')\n",
        "    while True:\n",
        "      decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "      for i in range(len(input_text)):\n",
        "        char = input_text[i]\n",
        "        if i>0 and i < len(decoded_sentence) and decoded_sentence[i] == '1':\n",
        "          print(' ',end='')\n",
        "          break\n",
        "        print(char, end='')\n",
        "      \n",
        "      if '1' not in decoded_sentence[1:]:\n",
        "        break\n",
        "      input_seq = input_seq[:, decoded_sentence[1:].index('1')+1:]\n",
        "      input_text = input_text[decoded_sentence[1:].index('1')+1:]\n",
        "      #print([input_seq, input_text])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: thefultoncountygrandjurysaidfridayaninvestigationofrecentprimaryelectionproducednoevidencethatanyirregularitiestookplace\n",
            "Decoded sentence:the fulton county grand jury said friday an investigation of recent primary election produced no evidence that anyir regularities took place\n",
            "-\n",
            "Input sentence: thejuryfurthersaidinpresentmentsthatthecityexecutivecommitteewhichhadchargeoftheelectiondeservesthepraiseandthanksofthecityofatlantaforthemannerinwhichtheelectionwasconducted\n",
            "Decoded sentence:the jury furthersaid in presentments that the city executive committee which had charge of the election deserves the praise and thanks of the city of at lanta for the manner in which the election was conducted\n",
            "-\n",
            "Input sentence: thetermjuryhadbeenchargedbyfultonsuperiorcourtjudgedurwoodpyetoinvestigatereportsofpossibleirregularitiesintheprimarywhichwaswonbyivanallen\n",
            "Decoded sentence:the term jury had been charged by fulton superior court judged urwood pyeto investigate reports of possible irregularities in the primary which was wonbyiva nallen\n",
            "-\n",
            "Input sentence: onlyarelativehandfulofsuchreportswasreceivedthejurysaidconsideringthewidespreadinterestintheelectionthenumberofvotersandthesizeofthiscity\n",
            "Decoded sentence:only a relative handful of such reports was received the jury said considering the wide spread interest in the election the number of voters and the size of this city\n",
            "-\n",
            "Input sentence: thejurysaiditdidfindthatmanyofregistrationandelectionlawsareoutmodedorinadequateandoftenambiguous\n",
            "Decoded sentence:the jury said it did find that many of registration and election laws are outmoded or in adequate and often ambiguous\n",
            "-\n",
            "Input sentence: itrecommendedthatfultonlegislatorsacttohavetheselawsstudiedandrevisedtotheendofmodernizingandimprovingthem\n",
            "Decoded sentence:it recommended that fulton legis lators act to have these laws studied and revised to the end of mode rnizing and improving them\n",
            "-\n",
            "Input sentence: thegrandjurycommentedonanumberofothertopicsamongthemtheatlantaandfultoncountypurchasingdepartmentswhichitsaidarewelloperatedandfollowgenerallyacceptedpracticeswhichinuretothebestinterestofbothgovernments\n",
            "Decoded sentence:the grand jury commented on a number of other to pics among them the at lanta and fulton county purchasing departments which it said a rewell operated and follow generally accepted practices which in ure to the best interest of both governments\n",
            "-\n",
            "Input sentence: howeverthejurysaiditbelievesthesetwoofficesshouldbecombinedtoachievegreaterefficiencyandreducethecostofadministration\n",
            "Decoded sentence:however the jury said it believes these two offices should be combined to a chieve greater efficiency and reduce the cost of administration\n",
            "-\n",
            "Input sentence: thecitypurchasingdepartmentthejurysaidislackinginexperiencedclericalpersonnelasaresultofcitypersonnelpolicies\n",
            "Decoded sentence:the city purchasing department the jury said i slacking in experienced clerical personnelas a result of city personnel policies\n",
            "-\n",
            "Input sentence: iturgedthatthecitytakestepstoremedythisproblem\n",
            "Decoded sentence:it urged that the city take steps to remedy this problem\n",
            "-\n",
            "Input sentence: implementationofautomobiletitlelawwasalsorecommendedbytheoutgoingjury\n",
            "Decoded sentence:implementation of automobile title law was also recommended by the out going jury\n",
            "-\n",
            "Input sentence: iturgedthatthenextlegislatureprovideenablingfundsandtheeffectivedatesothatanorderlyimplementationofthelawmaybeeffected\n",
            "Decoded sentence:it urged that the next legislature provide enabling funds and the effective date so that an or derly implementation of the law maybe effected\n",
            "-\n",
            "Input sentence: thegrandjurytookaswipeatthestatewelfarehandlingoffederalfundsgrantedforchildwelfareservicesinfosterhomes\n",
            "Decoded sentence:the grand jury took as wipe at the state welfare handling of federal funds granted for child welf a reservices in foster homes\n",
            "-\n",
            "Input sentence: thisisoneofthemajoritemsinthefultoncountygeneralassistanceprogramthejurysaidbutthestatewelfaredepartmenthasseenfittodistributethesefundsthroughthewelfaredepartmentsofallthecountiesinthestatewiththeexceptionoffultoncountywhichreceivesnoneofthismoney\n",
            "Decoded sentence:this is one of the majo ritems in the fulton county general assistance program the jury said but the state welf a redepartment has seen f it to distribute these funds through the welf a redepartments of all the counties in the state with the exception offul ton county which receive snone of this money\n",
            "-\n",
            "Input sentence: thejurorssaidtheyrealizeaproportionatedistributionofthesefundsmightdisablethisprograminourlesspopulouscounties\n",
            "Decoded sentence:the jurors said they realize a proportionate distribution of these fund smight disable this program in our less populous counties\n",
            "-\n",
            "Input sentence: neverthelesswefeelthatinthefuturefultoncountyshouldreceivesomeportionoftheseavailablefundsthejurorssaid\n",
            "Decoded sentence:nevertheless we feel that in the future fulton county should receive some portion of these available funds the jurors said\n",
            "-\n",
            "Input sentence: failuretodothiswillcontinuetoplaceadisproportionateburdenonfultontaxpayers\n",
            "Decoded sentence:failure to do this will continue to place a disproportionate burden on fulton taxpayers\n",
            "-\n",
            "Input sentence: thejuryalsocommentedonthefultoncourtwhichhasbeenunderfireforitspracticesintheappointmentofappraisersguardiansandadministratorsandtheawardingoffeesandcompensation\n",
            "Decoded sentence:the jury also commented on the fulton court which has been under fire for its practices in the appointment of appraisers guardians and administrators and the a warding offees and compensation\n",
            "-\n",
            "Input sentence: thejurysaiditfoundthecourthasincorporatedintoitsoperatingprocedurestherecommendationsoftwopreviousgrandjuriestheatlantabarassociationandaninterimcitizenscommittee\n",
            "Decoded sentence:the jury said it found the court has in corporated in to its operating procedures the recommendations of two previous grand juries the at lanta barassociation and an interim citizen scomm it tee\n",
            "-\n",
            "Input sentence: theseactionsshouldservetoprotectinfactandineffectthewardsfromunduecostsanditsappointedandelectedservantsfromunmeritoriouscriticismsthejurysaid\n",
            "Decoded sentence:these actions should serve to protect in fact and in effect the wards from undue costs and its appointed and elected servants from unmeritorious criticisms the jury said\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMDe3nY0W3Z"
      },
      "source": [
        "with open('story.txt', 'r') as file:\n",
        "  story = file.readline().replace('-','').replace(';', '').replace('(', '').replace(')', '').strip()\n",
        "\n",
        "# encoding test data (seperate to the training dataset)\n",
        "encoder_story = np.zeros((len(story), num_encoder_tokens), dtype=\"float32\")\n",
        "for t, char in enumerate(story):\n",
        "  encoder_story[t, input_token_index[char]] = 1.0\n",
        "encoder_story[t + 1 :, input_token_index[\" \"]] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrKpu2MuvKUB",
        "outputId": "df51eb5f-da93-4da2-de67-dcf155d4f6bd"
      },
      "source": [
        "# predicting story.txt and saving the output\n",
        "if 1:\n",
        "    input_seq = encoder_story.reshape((1, encoder_story.shape[0], encoder_story.shape[1]))\n",
        "    input_text = story\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_text)\n",
        "    print(\"Decoded sentence:\", end='')\n",
        "    storylist = []\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "      except:\n",
        "        break\n",
        "\n",
        "      for i in range(len(input_text)):\n",
        "        char = input_text[i]\n",
        "        if i>0 and i < len(decoded_sentence) and decoded_sentence[i] == '1':\n",
        "          print(' ',end='')\n",
        "          storylist.append(' ')\n",
        "          break\n",
        "        print(char, end='')\n",
        "        storylist.append(char)\n",
        "      \n",
        "      if '1' not in decoded_sentence[1:]:\n",
        "        break\n",
        "      input_seq = input_seq[:, decoded_sentence[1:].index('1')+1:]\n",
        "      input_text = input_text[decoded_sentence[1:].index('1')+1:]\n",
        "      #print([input_seq, input_text])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: northrichmondstreetbeingblindwasaquietstreetexceptatthehourwhenthechristianbrothersschoolsettheboysfreeanuninhabitedhouseoftwostoreysstoodattheblindenddetachedfromitsneighboursinasquaregroundtheotherhousesofthestreetconsciousofdecentliveswithinthemgazedatoneanotherwithbrownimperturbablefacestheformertenantofourhouseapriesthaddiedinthebackdrawingroomairmustyfromhavingbeenlongenclosedhunginalltheroomsandthewasteroombehindthekitchenwaslitteredwitholduselesspapersamongtheseifoundafewpapercoveredbooksthepagesofwhichwerecurledanddamptheabbotbywalterscottthedevoutcommunicantandthememoirsofvidocqilikedthelastbestbecauseitsleaveswereyellowthewildgardenbehindthehousecontainedacentralappletreeandafewstragglingbushesunderoneofwhichifoundthelatetenantsrustybicyclepumphehadbeenaverycharitablepriestinhiswillhehadleftallhismoneytoinstitutionsandthefurnitureofhishousetohissisterwhentheshortdaysofwintercameduskfellbeforewehadwelleatenourdinnerswhenwemetinthestreetthehouseshadgrownsombrethespaceofskyaboveuswasthecolourofeverchangingvioletandtowardsitthelampsofthestreetliftedtheirfeeblelanternsthecoldairstungusandweplayedtillourbodiesglowedourshoutsechoedinthesilentstreetthecareerofourplaybroughtusthroughthedarkmuddylanesbehindthehouseswhereweranthegauntletoftheroughtribesfromthecottagestothebackdoorsofthedarkdrippinggardenswhereodoursarosefromtheashpitstothedarkodorousstableswhereacoachmansmoothedandcombedthehorseorshookmusicfromthebuckledharnesswhenwereturnedtothestreetlightfromthekitchenwindowshadfilledtheareasifmyunclewasseenturningthecornerwehidintheshadowuntilwehadseenhimsafelyhousedorifmanganssistercameoutonthedoorsteptocallherbrotherintohisteawewatchedherfromourshadowpeerupanddownthestreetwewaitedtoseewhethershewouldremainorgoinandifsheremainedweleftourshadowandwalkeduptomangansstepsresignedlyshewaswaitingforusherfiguredefinedbythelightfromthehalfopeneddoorherbrotheralwaysteasedherbeforeheobeyedandistoodbytherailingslookingatherherdressswungasshemovedherbodyandthesoftropeofherhairtossedfromsidetosideeverymorningilayonthefloorinthefrontparlourwatchingherdoortheblindwaspulleddowntowithinaninchofthesashsothaticouldnotbeseenwhenshecameoutonthedoorstepmyheartleapedirantothehallseizedmybooksandfollowedherikeptherbrownfigurealwaysinmyeyeandwhenwecamenearthepointatwhichourwaysdivergediquickenedmypaceandpassedherthishappenedmorningaftermorningihadneverspokentoherexceptforafewcasualwordsandyethernamewaslikeasummonstoallmyfoolishbloodherimageaccompaniedmeeveninplacesthemosthostiletoromanceonsaturdayeveningswhenmyauntwentmarketingihadtogotocarrysomeoftheparcelswewalkedthroughtheflaringstreetsjostledbydrunkenmenandbargainingwomenamidthecursesoflabourerstheshrilllitaniesofshopboyswhostoodonguardbythebarrelsofpigscheeksthenasalchantingofstreetsingerswhosangacomeallyouaboutodonovanrossaoraballadaboutthetroublesinournativelandthesenoisesconvergedinasinglesensationoflifeformeiimaginedthatiboremychalicesafelythroughathrongoffoeshernamesprangtomylipsatmomentsinstrangeprayersandpraiseswhichimyselfdidnotunderstandmyeyeswereoftenfulloftearsicouldnottellwhyandattimesafloodfrommyheartseemedtopouritselfoutintomybosomithoughtlittleofthefutureididnotknowwhetheriwouldeverspeaktoherornotorifispoketoherhowicouldtellherofmyconfusedadorationbutmybodywaslikeaharpandherwordsandgestureswerelikefingersrunninguponthewiresoneeveningiwentintothebackdrawingroominwhichthepriesthaddieditwasadarkrainyeveningandtherewasnosoundinthehousethroughoneofthebrokenpanesiheardtherainimpingeupontheearththefineincessantneedlesofwaterplayinginthesoddenbedssomedistantlamporlightedwindowgleamedbelowmeiwasthankfulthaticouldseesolittleallmysensesseemedtodesiretoveilthemselvesandfeelingthatiwasabouttoslipfromthemipressedthepalmsofmyhandstogetheruntiltheytrembledmurmuringoloveolovemanytimesatlastshespoketomewhensheaddressedthefirstwordstomeiwassoconfusedthatididnotknowwhattoanswersheaskedmewasigoingtoarabyiforgotwhetheriansweredyesornoitwouldbeasplendidbazaarshesaidshewouldlovetogoandwhycantyouiaskedwhileshespokesheturnedasilverbraceletroundandroundherwristshecouldnotgoshesaidbecausetherewouldbearetreatthatweekinherconventherbrotherandtwootherboyswerefightingfortheircapsandiwasaloneattherailingssheheldoneofthespikesbowingherheadtowardsmethelightfromthelampoppositeourdoorcaughtthewhitecurveofhernecklitupherhairthatrestedthereandfallinglitupthehandupontherailingitfelloveronesideofherdressandcaughtthewhiteborderofapetticoatjustvisibleasshestoodateaseitswellforyoushesaidifigoisaidiwillbringyousomethingwhatinnumerablefollieslaidwastemywakingandsleepingthoughtsafterthateveningiwishedtoannihilatethetediousinterveningdaysichafedagainsttheworkofschoolatnightinmybedroomandbydayintheclassroomherimagecamebetweenmeandthepageistrovetoreadthesyllablesofthewordarabywerecalledtomethroughthesilenceinwhichmysoulluxuriatedandcastaneasternenchantmentovermeiaskedforleavetogotothebazaaronsaturdaynightmyauntwassurprisedandhopeditwasnotsomefreemasonaffairiansweredfewquestionsinclassiwatchedmymastersfacepassfromamiabilitytosternnesshehopediwasnotbeginningtoidleicouldnotcallmywanderingthoughtstogetherihadhardlyanypatiencewiththeseriousworkoflifewhichnowthatitstoodbetweenmeandmydesireseemedtomechildsplayuglymonotonouschildsplayonsaturdaymorningiremindedmyunclethatiwishedtogotothebazaarintheeveninghewasfussingatthehallstandlookingforthehatbrushandansweredmecurtlyyesboyiknowashewasinthehallicouldnotgointothefrontparlourandlieatthewindowileftthehouseinbadhumourandwalkedslowlytowardstheschooltheairwaspitilesslyrawandalreadymyheartmisgavemewhenicamehometodinnermyunclehadnotyetbeenhomestillitwasearlyisatstaringattheclockforsometimeandwhenitstickingbegantoirritatemeilefttheroomimountedthestaircaseandgainedtheupperpartofthehousethehighcoldemptygloomyroomsliberatedmeandiwentfromroomtoroomsingingfromthefrontwindowisawmycompanionsplayingbelowinthestreettheircriesreachedmeweakenedandindistinctandleaningmyforeheadagainstthecoolglassilookedoveratthedarkhousewhereshelivedimayhavestoodthereforanhourseeingnothingbutthebrowncladfigurecastbymyimaginationtoucheddiscreetlybythelamplightatthecurvedneckatthehandupontherailingsandattheborderbelowthedresswhenicamedownstairsagainifoundmrsmercersittingatthefireshewasanoldgarrulouswomanapawnbrokerswidowwhocollectedusedstampsforsomepiouspurposeihadtoendurethegossipoftheteatablethemealwasprolongedbeyondanhourandstillmyuncledidnotcomemrsmercerstooduptogoshewassorryshecouldntwaitanylongerbutitwasaftereightoclockandshedidnotliketobeoutlateasthenightairwasbadforherwhenshehadgoneibegantowalkupanddowntheroomclenchingmyfistsmyauntsaidimafraidyoumayputoffyourbazaarforthisnightofourlordatnineoclockiheardmyuncleslatchkeyinthehalldooriheardhimtalkingtohimselfandheardthehallstandrockingwhenithadreceivedtheweightofhisovercoaticouldinterpretthesesignswhenhewasmidwaythroughhisdinneriaskedhimtogivemethemoneytogotothebazaarhehadforgottenthepeopleareinbedandaftertheirfirstsleepnowhesaidididnotsmilemyauntsaidtohimenergeticallycantyougivehimthemoneyandlethimgoyouvekepthimlateenoughasitismyunclesaidhewasverysorryhehadforgottenhesaidhebelievedintheoldsayingallworkandnoplaymakesjackadullboyheaskedmewhereiwasgoingandwhenitoldhimasecondtimeheaskedmedidiknowthearabsfarewelltohissteedwhenileftthekitchenhewasabouttorecitetheopeninglinesofthepiecetomyauntiheldaflorintightlyinmyhandasistrodedownbuckinghamstreettowardsthestationthesightofthestreetsthrongedwithbuyersandglaringwithgasrecalledtomethepurposeofmyjourneyitookmyseatinathirdclasscarriageofadesertedtrainafteranintolerabledelaythetrainmovedoutofthestationslowlyitcreptonwardamongruinoushousesandoverthetwinklingriveratwestlandrowstationacrowdofpeoplepressedtothecarriagedoorsbuttheportersmovedthembacksayingthatitwasaspecialtrainforthebazaariremainedaloneinthebarecarriageinafewminutesthetraindrewupbesideanimprovisedwoodenplatformipassedoutontotheroadandsawbythelighteddialofaclockthatitwastenminutestoteninfrontofmewasalargebuildingwhichdisplayedthemagicalnameicouldnotfindanysixpennyentranceandfearingthatthebazaarwouldbeclosedipassedinquicklythroughaturnstilehandingashillingtoawearylookingmanifoundmyselfinabighallgirdedathalfitsheightbyagallerynearlyallthestallswereclosedandthegreaterpartofthehallwasindarknessirecognizedasilencelikethatwhichpervadesachurchafteraserviceiwalkedintothecentreofthebazaartimidlyafewpeopleweregatheredaboutthestallswhichwerestillopenbeforeacurtainoverwhichthewordscafchantantwerewrittenincolouredlampstwomenwerecountingmoneyonasalverilistenedtothefallofthecoinsrememberingwithdifficultywhyihadcomeiwentovertooneofthestallsandexaminedporcelainvasesandfloweredteasetsatthedoorofthestallayoungladywastalkingandlaughingwithtwoyounggentlemeniremarkedtheirenglishaccentsandlistenedvaguelytotheirconversationoineversaidsuchathingobutyoudidobutididntdidntshesaythatyesiheardherotheresafibobservingmetheyoungladycameoverandaskedmedidiwishtobuyanythingthetoneofhervoicewasnotencouragingsheseemedtohavespokentomeoutofasenseofdutyilookedhumblyatthegreatjarsthatstoodlikeeasternguardsateithersideofthedarkentrancetothestallandmurmurednothankyoutheyoungladychangedthepositionofoneofthevasesandwentbacktothetwoyoungmentheybegantotalkofthesamesubjectonceortwicetheyoungladyglancedatmeoverhershoulderilingeredbeforeherstallthoughiknewmystaywasuselesstomakemyinterestinherwaresseemthemorerealtheniturnedawayslowlyandwalkeddownthemiddleofthebazaariallowedthetwopenniestofallagainstthesixpenceinmypocketiheardavoicecallfromoneendofthegallerythatthelightwasouttheupperpartofthehallwasnowcompletelydarkgazingupintothedarknessisawmyselfasacreaturedrivenandderidedbyvanityandmyeyesburnedwithanguishandanger\n",
            "Decoded sentence:north richmond street being blind was a quiet street except at the hour when the christian brothers school set the boys freean uninhabited house of two storeys stood at the blindend detached from it sneigh bours in a square ground the other houses of the street conscious of decent lives within the mgazed at one another with brown imperturbable faces the formerten ant of our houseapriest had died in the backdrawing roomair musty from having been long enclosed hung in all the rooms and the waste room behind the kitchen was littered with olduseless papers among these i found a few paper covered books the pages of which we recurled and damp the abbot by walter scott the devout communicant and the memoirs of vidocqiliked the last best because its leaves were yel low the wild garden behind the house contained a central appletree and a few straggling bushe sunder one of which i found the latetenants rusty bicycle pumphe had been a very charitable priest in his will he had leftall his money to institutions and the furniture of his house to his sister when the short days of winter came dusk fell before we hadwell eaten our dinners when we met in the street the houses had grown sombret he space of skya boveus was the colour of everchanging violet and towards it the lamps of the street lifted their feeblel anternsthe cold airstung us and we played till ourbodies glowed our shout sechoed in the silent street the career of our play brought us throught he dark muddy lanes behind the houses where we ran the gaunt let of the rough tribes from the cottages to the back doors of the dark dripping gardens where odours a rose from the ash pits to the darkodo rous stables where a coach mans moothed and combed the horse or shook music from the buckled harness when we returned to the street light from the kitchen windows had filled the areas if my uncle wasseen turning the corner we hid in the shadow until we had seen himsafely housed or if manganssister came out on the door step to call her brother in to his teawew atched her from our shadowpeer up and down the street we waited to see whether she would rem a in orgo in and if she remained we left our shadow and walked up tomang ans steps resignedly she was waiting for usher figure defined by the light from the half opened door her brother always teased her before he obeyed and i stood by the railing slooking a ther her dress swung as she moved her body and the soft rope of her hair tossed from side to side every morning i lay on the floor in the front parlour watching her door the blind was pulled down to within aninch of the sash so that i could not be seen when she came out on the door step my heart leape diran to the hall seized my books and followed he rikepther brown figure always in my e yea ndwhen we came near the pointatw hichour ways diverge diquickened my pace and passed her this happened morning after morning i had never spoken to her except for a few casual words and yet her name was like a summons toall my foolish blood her image accompanied me even in places the most hostile to romance on saturday evenings when my a untwent marketing i had to go to carry some of the parcels we walked through the flaring streets jostled by drunken men and bargaining wome namid the curses of labourers the shrill litanies of shop boys whostood on guard by the barrels of pigscheeks then a sal chanting of stree tsingers whos ang a come all you aboutod on ovanross aor a ballad about the troubles in our native landthese no ises converged in a single sensation of life for meiimagined that i bore my chalices a fely through a throng of foes her name sprang to my lips at moments instrange prayers and praises which i myselfd idnot understand my eyes were often full of tears i could not tell why and at times af lood from my he art seemed to pour itself out in to my bosom i thought little of the future i did not know whether i would ever speak to he ror not or if ispoke to her how i could tell he rofmy confused adoration but my body was like a harp and her words and gestures were like fingers running upon the wires one evening i went in to the back draw in groom in which the priest had died it was adark rainy evening and there was no sound in the housethrough one of the brokenp anes i heard the rain impinge upon the earth the fine incessant needles of water playing in the sodden bedssome distant lamporlighted windowg leamed below me i wast hankful that i could see so little all my sensesseemed to desire toveil themselves and feeling tha tiwas about to slipfrom the mipressed the palms of my hands together until they trembled murmuringo loveolove many times at last she spoke tomewhen she addressed the first words to me i wasso confused that i did not know what to answers he asked mewas i going to araby i for got whet her i answered yes or no it would be a splendid bazaars he said she would love to go and why cant you i asked whiles he spokes he turned a silver brace let round and round her wrists he could not goshesaid because there would be a retreat that week in her conventher brother and two other boys were fighting for their caps and i was alone at the railings she held one of the spikes bowing her head towards me the lightfrom the lampopposite our door caught the white curve of her neck litup her hair that rested there and falling lit upthe hand upon the railing it fell over one side of her dress and caught the white border of a pettic oatjust visible as she stood at ease its well for you she said i figo i said i will bring you something what innumerable follies laid was temy waking and sleeping thoughts after that evening i wished to annihilate the tedious intervening daysic hafed against the work of school at night in my bedroom and by day in the classroom her i mage came between me and the page i strove to read the syllables of the word a raby we recalled tome through the silence in which my soull uxuriated and cast an eastern en chantment over meiasked forleave to go to the b a zaar on saturday night my aunt was surprised and hoped it was not some freemason affairian swered few questions in class i watched my masters face pass from amiability to sternnes she hoped i was not beginning toidle i could not call my wandering thoughts together i had hardly any patience with the serious work of life which now that its tood between meand my desire seemed to me child splayugly mono to nouschild splayon saturday morning i reminded my uncle that i wished to go to the bazaar in the evening he was fussing at the hall stand looking for the hat brush and answered mecurtly yes boyi knowas he was in the hall i could not go in to the front parlour and lie at the windo wileft the house in bad humour and walked slowly towards the school the air was pitilessly raw and already my heartm is gave mewhenicame home to dinner my uncle had not yet been home still it was early is at staring at the clock for sometime and when it sticking beganto irritate meileft the room i mounted the stair case and gained the upper part of the house the high coldempty gloomy rooms liberated me and i went from room to roomsinging from the front window i sawmy companions playing below in the street their cries reached me weakened and indistinct and leaning my forehead against the cool glassilooked over at the dark house where she lived i may have stood there for an hour seeing nothing but the brown clad figure cast by my imagination touched discreetly by the lamplight at the curved neck at the hand upon the railings and at the border below the dress when i came down stairs again i found mrsmercersitting at the fires he was an old gar rulous woman a pawn brokers widow who collected used stamps for somepious purpose i had to endure the gossip of the teat a ble the meal was prolonged beyond an hour and stillmy uncle did not comemrsmercer stood up to go she was sorry she couldnt wait any longer but it was after eigh to clock and she did not like to be outlate as the nightair was badfor her when she had gone i began to walk up and down the room clenching my fist smyaunt sai dima fraid you mayputoff your bazaar for this night of our lord at nineoclock i heard my uncle slatchkey in the halldoor i heard him talking to himself and heard the hall stand rocking when it had received the weight of his overcoat i could interpret these signs when he was midway through his dinneriasked him to give me the money to go to the bazaar he had forgotten the people arein bed and after their first slee pnow he said i did nots milemyaunt said to him energetically cant you give him the money and let him go you vekept him late enoug has it i smyuncle said he was very sorry he had forgotten he said he believed in the old saying all work and no play makes jack a dull boyhe asked me where i was going and when i told him a second time he asked me didik now the ar absfare well to his steed when i left the kitchen he was about to recite the opening lines of the piece tomy a untiheld a flor intightly in my hand as i strode down bucking hams treet towards the station the sight of the streets thronged with buyers and glaring with gas recalled to me the purpose of my journey i took my seat in a third class carriage of a deserted train after an in tolerable delay the train moved out of the station slowly it crepton ward among ruinous houses and over the twinkling river a twe stlandrow station a crowd of people pressed to the carriage doors but the porters moved them back saying that it was a special train for the bazaarir emained alone in the bare carriage in a few minutes the traindrew up beside an improvised wooden platformipassed out on to the roadand saw by the lighted dial of a clock that it wasten minutes toten in front of me was a large building which displayed the magical name i could not find any six pennyentrance and fearing that the bazaar would be closed i passed in quickly through a turnstile handing a shilling to a weary looking manifound myselfina big hall girded at half it sheight by a gallery nearly all the stalls we reclosed and the greater part of the hall was indark nessire cognized a silence like that which per vades a church after a service i walked in to the centre of the baz a artimidly a few people we regathered about the stalls which were still open before a curtain over which the wordscaf chantant were written incoloured lampstwo men were counting money on as alveril i stened to the fall of the coinsremembering with difficulty why i had come i went over to one of the stalls and examined porce lainvases and flowered teasets at the door of the stall a young lady was talking and laughing with two young gentlemenire marked their english accents and listened vaguely to their conversatio no i never said such a thin go but you didobutididnt didnt she say that yes i heard he rothere saf i bobs erving me the young lady came over and asked me didiwish tobuy anything the tone of hervoice was not encouraging she seemed to have spokent ome out of a sense of dutyilo oked humbly at the great jars that stood like eastern guards at either side of the darkentrance to the stall and murmured nothank you the young lady changed the position of one of the vases and went back to the two young men they began to talk of the same subjectonce ortwice the young lady glanced at me over her shoulderilingered before her stall though i knew my stay was useless to makemy interest in her wares seem the morereal the niturned away slowly and walked down the middle of the bazaarial lowed the two penniest of all against the sixpence in my pocket i heard a voic ecall from one end of the gallery that the light was out the upper part of the hall was now completely dark gazing upin to the darkness i saw myself as a creatured riven and derided by vanity and my eyes burned with anguish and anger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqeBPT_0IC5s"
      },
      "source": [
        "story_txt = ''.join(storylist)\n",
        "with open('story2.txt', 'w') as f:\n",
        "    f.write(story_txt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}